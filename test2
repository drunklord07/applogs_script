#!/usr/bin/env python3
import re
import json
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE  = "input.txt"
OUTPUT_FILE = "ParsedLogs_Test.xlsx"

# Timestamp detection (with optional milliseconds)
TIMESTAMP_RE = re.compile(
    r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:,\d+)?\s'
)

def detect_type(line):
    if ' cid=[' in line and ' txn=[' in line:
        return 'A'
    if '~#~' in line:
        return 'B'
    if 'UKC:' in line:
        return 'C'
    return 'Unknown'

def parse_type_a(line):
    m = re.match(
        r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:,\d+)?)\s+'
        r'(?P<level>\w+)\s+\[(?P<thread>[^\]]+)\]\s+'
        r'cid=\[(?P<cid>[^\]]*)\]\s+'
        r'txn=\[(?P<txn>[^\]]*)\]\s+'
        r'(?P<logger>\w+)\s*:\s*(?P<message>.+)$',
        line
    )
    return m.groupdict() if m else {"parse_error": "Type A failed"}

def parse_type_b(lines):
    # Combine all lines of this log
    full = "\n".join(txt for _, txt in lines)
    # Extract timestamp, thread, level, logger from first line
    first = lines[0][1]
    ts_m = re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+)', first)
    timestamp = ts_m.group(1) if ts_m else ""
    br = re.search(r'\[([^\]]+)\]\s*\[([^\]]+)\]\s*\[([^\]]+)\]', first)
    thread, level, logger = (br.group(1), br.group(2).strip(), br.group(3)) if br else ("","","")
    # Split on the LAST "~#~" occurrence
    if "~#~" in full:
        metadata, payload = full.rsplit("~#~", 1)
    else:
        metadata, payload = full, ""
    # Remove any leading "\n" from payload
    payload = payload.lstrip("\n")
    # Attempt to flatten JSON in payload
    flat = {}
    try:
        obj = json.loads(payload)
        def _flatten(o, pre=""):
            out = {}
            if isinstance(o, dict):
                for k,v in o.items():
                    key = f"{pre}{k}"
                    if isinstance(v, (dict, list)):
                        out.update(_flatten(v, key + "."))
                    else:
                        out[key] = v
            elif isinstance(o, list):
                for i,item in enumerate(o):
                    out.update(_flatten(item, f"{pre}[{i}]."))
            return out
        flat = _flatten(obj)
    except:
        pass

    rec = {
        "timestamp": timestamp,
        "thread":    thread,
        "level":     level,
        "logger":    logger,
        "metadata":  metadata,
        "payload":   payload
    }
    # Merge any flattened JSON fields
    for k,v in flat.items():
        rec[f"json.{k}"] = v
    return rec

def parse_type_c(line):
    m = re.match(
        r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+)\s+'
        r'\[(?P<thread>[^\]]+)\]\s+'
        r'\[(?P<level>\w+)\s*\]\s+'
        r'\[(?P<logger>[^\]]+)\]\s*-\s*(?P<rest>.+)$',
        line
    )
    if not m:
        return {"parse_error": "Type C failed"}
    info = m.groupdict()
    parts = [p.strip() for p in info['rest'].split(',')]
    rec = {
        "timestamp":    info['timestamp'],
        "thread":       info['thread'],
        "level":        info['level'],
        "logger":       info['logger'],
    }
    rec["ukc"]           = parts[0].split(':',1)[1] if parts and ':' in parts[0] else parts[0] if parts else ""
    rec["request_time"]  = parts[1] if len(parts)>1 else ""
    rec["response_time"] = parts[2] if len(parts)>2 else ""
    rec["masked_id"]     = parts[3] if len(parts)>3 else ""
    rec["operation"]     = parts[4] if len(parts)>4 else ""
    rec["other"]         = ",".join(parts[5:]) if len(parts)>5 else ""
    return rec

def main():
    # 1) Read all non-empty lines with their numbers
    raw = []
    with open(INPUT_FILE, encoding='utf-8', errors='ignore') as f:
        for idx, ln in enumerate(f, start=1):
            txt = ln.rstrip('\n')
            if txt.strip():
                raw.append((idx, txt))

    # 2) Group into multi-line logs by timestamp at start
    logs = []
    for ln_no, txt in raw:
        if TIMESTAMP_RE.match(txt):
            logs.append({"start_line": ln_no, "lines": [(ln_no, txt)]})
        else:
            if logs:
                logs[-1]["lines"].append((ln_no, txt))

    # 3) Parse each log
    records = []
    for rec in tqdm(logs, desc="Parsing logs"):
        ln0 = rec["start_line"]
        if detect_type(rec["lines"][0][1]) == 'A':
            data = parse_type_a(rec["lines"][0][1])
        elif detect_type(rec["lines"][0][1]) == 'B':
            data = parse_type_b(rec["lines"])
        elif detect_type(rec["lines"][0][1]) == 'C':
            data = parse_type_c(rec["lines"][0][1])
        else:
            data = {"parse_error": "Unknown type"}
        data["line_no"] = ln0
        data["type"]    = detect_type(rec["lines"][0][1])
        records.append(data)

    # 4) Write to Excel for validation
    wb = xlsxwriter.Workbook(OUTPUT_FILE)
    ws = wb.add_worksheet("ParsedLogs")
    # Dynamic headers
    headers = []
    for row in records:
        for key in row:
            if key not in headers:
                headers.append(key)
    for col, h in enumerate(headers):
        ws.write(0, col, h)
    for r, row in enumerate(records, start=1):
        for c, h in enumerate(headers):
            ws.write(r, c, row.get(h, ""))
    wb.close()

    print(f"Validation workbook '{OUTPUT_FILE}' created with {len(records)} rows.")

if __name__ == "__main__":
    main()
