#!/usr/bin/env python3
import re
import json
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
OUTPUT_XLSX  = "ParsedLogs.xlsx"
OUTPUT_DOCX  = "ParsedLogs.docx"

MOBILE_REGEX = re.compile(r'(?<!\d)((?:\+91[\-\s]?|91[\-\s]?|0)?[6-9]\d{9})(?!\d)')
TIMESTAMP_RE = re.compile(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:,\d+)?\s')

# === HELPERS ===
def detect_type(line):
    if ' cid=[' in line and ' txn=[' in line:
        return 'A'
    if '~#' in line:
        return 'B'
    if 'UKC:' in line:
        return 'C'
    return None

def parse_type_a(line):
    m = re.match(
        r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})(?:,\d+)?\s+'
        r'(?P<level>\w+)\s+\[(?P<thread>[^\]]+)\]\s+'
        r'cid=\[(?P<cid>[^\]]*)\]\s+'
        r'txn=\[(?P<txn>[^\]]*)\]\s+'
        r'(?P<logger>\w+)\s*:\s*(?P<message>.+)$',
        line
    )
    if not m:
        raise ValueError("Type A parse error")
    return m.groupdict()

def parse_type_b(meta_line, rest_lines):
    # 1) line number and timestamp done upstream
    # 2) extract timestamp, thread, level, logger
    ts_m = re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+)', meta_line)
    timestamp = ts_m.group(1) if ts_m else ""
    br = re.match(r'^[^\[]*\[([^\]]+)\]\s*\[([^\]]+)\]\s*\[([^\]]+)\]', meta_line)
    thread, level, logger = (br.group(1), br.group(2).strip(), br.group(3)) if br else ("","","")
    # 3) metadata = everything after "] - "
    meta_part = meta_line.split('] - ',1)[1]
    # 4) payload = all lines prefixed with "~#"; strip that prefix
    payload_lines = [txt.lstrip('~#') for _,txt in rest_lines if txt.startswith('~#')]
    payload = "\n".join(payload_lines)

    return {
        "timestamp": timestamp,
        "thread":    thread,
        "level":     level,
        "logger":    logger,
        "metadata":  meta_part,
        "payload":   payload
    }

def parse_type_c(line):
    m = re.match(
        r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+)\s+'
        r'\[(?P<thread>[^\]]+)\]\s+'
        r'\[(?P<level>\w+)\s*\]\s+'
        r'\[(?P<logger>[^\]]+)\]\s*-\s*(?P<rest>.+)$',
        line
    )
    if not m:
        raise ValueError("Type C parse error")
    info = m.groupdict()
    parts = [p.strip() for p in info['rest'].split(',')]
    return {
        "timestamp":     info['timestamp'],
        "thread":        info['thread'],
        "level":         info['level'],
        "logger":        info['logger'],
        "ukc":           parts[0].split(':',1)[1] if ':' in parts[0] else parts[0],
        "request_time":  parts[1] if len(parts)>1 else "",
        "response_time": parts[2] if len(parts)>2 else "",
        "masked_id":     parts[3] if len(parts)>3 else "",
        "operation":     parts[4] if len(parts)>4 else "",
        "other":         ",".join(parts[5:]) if len(parts)>5 else ""
    }

# === MAIN PROCESS ===
def main():
    # 1) read non-blank lines
    raw = []
    with open(INPUT_FILE, encoding='utf-8', errors='ignore') as f:
        for num, ln in enumerate(f, start=1):
            txt = ln.rstrip('\n')
            if txt.strip():
                raw.append((num, txt))

    # 2) group into logs by timestamp start
    logs = []
    for ln_no, txt in raw:
        if TIMESTAMP_RE.match(txt):
            logs.append({"start_line": ln_no, "lines": [(ln_no, txt)]})
        else:
            if logs:
                logs[-1]["lines"].append((ln_no, txt))

    total_logs = len(logs)
    parsed     = []
    errors     = []

    # 3) parse each log
    for rec in tqdm(logs, desc="Parsing logs"):
        ln0, meta = rec["lines"][0]
        rest      = rec["lines"][1:]
        ttype     = detect_type(meta)
        try:
            if ttype=='A':
                data = parse_type_a(meta)
            elif ttype=='B':
                data = parse_type_b(meta, rest)
            elif ttype=='C':
                data = parse_type_c(meta)
            else:
                raise ValueError("Unknown log type")
        except Exception as e:
            errors.append({
                "line_no": ln0,
                "raw":     "\n".join(l for _,l in rec["lines"]),
                "error":   str(e)
            })
            continue

        # scan for mobiles
        for k,v in data.items():
            for m in MOBILE_REGEX.finditer(str(v)):
                parsed.append({
                    "line_no":  rec["start_line"],
                    "type":     ttype,
                    **data,
                    "match":    m.group(1),
                    "field":    k
                })

    # 4) write Excel
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    # ParsedLogs sheet
    ws = wb.add_worksheet("ParsedLogs")
    if parsed:
        # dynamic headers
        hdrs = []
        for row in parsed:
            for h in row:
                if h not in hdrs:
                    hdrs.append(h)
        for c,h in enumerate(hdrs):
            ws.write(0, c, h)
        red = wb.add_format({"font_color":"red"})
        for r,row in enumerate(parsed, start=1):
            for c,h in enumerate(hdrs):
                val = row.get(h,"")
                fmt = red if h=="match" else None
                ws.write(r, c, val, fmt)
    # Errors sheet
    ws2 = wb.add_worksheet("Errors")
    ws2.write_row(0,0,["line_no","raw","error"])
    for r,err in enumerate(errors, start=1):
        ws2.write(r,0,err["line_no"])
        ws2.write(r,1,err["raw"])
        ws2.write(r,2,err["error"])
    wb.close()

    # 5) write Word
    doc = Document()
    for row in tqdm(parsed, desc="Generating Word"):
        p = doc.add_paragraph()
        p.add_run(f"{row['line_no']} | {row['timestamp']} | ")
        match_run = p.add_run(row['match'])
        match_run.font.color.rgb = RGBColor(255,0,0)
        p.add_run(f"  [{row['field']}]")
    doc.save(OUTPUT_DOCX)

    # 6) summary
    print(f"Total logs           : {total_logs}")
    print(f"Total mobile matches : {len(parsed)}")
    print(f"Parse errors         : {len(errors)}")

if __name__=="__main__":
    main()
