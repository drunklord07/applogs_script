#!/usr/bin/env python3
import re
import json
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
OUTPUT_FILE  = "ParsedLogs_Test.xlsx"

# Timestamp detection (with optional milliseconds)
TIMESTAMP_RE = re.compile(
    r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:,\d+)?\s'
)

def detect_type(line):
    if ' cid=[' in line and ' txn=[' in line:
        return 'A'
    if '~#~' in line:
        return 'B'
    if 'UKC:' in line:
        return 'C'
    return 'Unknown'

def parse_type_a(line):
    m = re.match(
        r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:,\d+)?)\s+'
        r'(?P<level>\w+)\s+\[(?P<thread>[^\]]+)\]\s+'
        r'cid=\[(?P<cid>[^\]]*)\]\s+'
        r'txn=\[(?P<txn>[^\]]*)\]\s+'
        r'(?P<logger>\w+)\s*:\s*(?P<message>.+)$',
        line
    )
    return m.groupdict() if m else {"parse_error": "Type A failed"}

def parse_type_b(lines):
    meta_line = lines[0][1]
    # timestamp, thread, level, logger
    ts_m = re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+)', meta_line)
    timestamp = ts_m.group(1) if ts_m else ""
    br = re.search(r'\[([^\]]+)\]\s*\[([^\]]+)\]\s*\[([^\]]+)\]', meta_line)
    thread, level, logger = (br.group(1), br.group(2).strip(), br.group(3)) if br else ("","","")
    # metadata after "] - "
    metadata = meta_line.split('] - ',1)[1] if '] - ' in meta_line else ""
    # payload lines prefixed "~#~"
    payload = "\n".join(txt[3:] for _, txt in lines[1:] if txt.startswith('~#~'))
    # try flatten JSON
    flat = {}
    try:
        obj = json.loads(payload)
        def _flatten(o, pre=""):
            out = {}
            if isinstance(o, dict):
                for k,v in o.items():
                    key = f"{pre}{k}"
                    if isinstance(v,(dict,list)):
                        out.update(_flatten(v, key+"."))
                    else:
                        out[key] = v
            elif isinstance(o,list):
                for i,item in enumerate(o):
                    out.update(_flatten(item, f"{pre}[{i}]."))
            return out
        flat = _flatten(obj)
    except:
        pass

    rec = {
        "timestamp": timestamp,
        "thread":    thread,
        "level":     level,
        "logger":    logger,
        "metadata":  metadata,
        "payload":   payload
    }
    # merge flattened JSON
    for k,v in flat.items():
        rec[f"json.{k}"] = v
    return rec

def parse_type_c(line):
    m = re.match(
        r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+)\s+'
        r'\[(?P<thread>[^\]]+)\]\s+'
        r'\[(?P<level>\w+)\s*\]\s+'
        r'\[(?P<logger>[^\]]+)\]\s*-\s*(?P<rest>.+)$',
        line
    )
    if not m:
        return {"parse_error": "Type C failed"}
    info = m.groupdict()
    parts = [p.strip() for p in info['rest'].split(',')]
    rec = {
        "timestamp":    info['timestamp'],
        "thread":       info['thread'],
        "level":        info['level'],
        "logger":       info['logger'],
    }
    rec["ukc"]           = parts[0].split(':',1)[1] if ':' in parts[0] else parts[0]
    rec["request_time"]  = parts[1] if len(parts)>1 else ""
    rec["response_time"] = parts[2] if len(parts)>2 else ""
    rec["masked_id"]     = parts[3] if len(parts)>3 else ""
    rec["operation"]     = parts[4] if len(parts)>4 else ""
    rec["other"]         = ",".join(parts[5:]) if len(parts)>5 else ""
    return rec

def main():
    # 1. Load all non-empty lines with their numbers
    raw = []
    with open(INPUT_FILE, encoding='utf-8', errors='ignore') as f:
        for idx, ln in enumerate(f, start=1):
            txt = ln.rstrip('\n')
            if txt.strip():
                raw.append((idx, txt))

    # 2. Group into logs by timestamp at start
    logs = []
    for ln_no, txt in raw:
        if TIMESTAMP_RE.match(txt):
            logs.append({"start_line": ln_no, "lines": [(ln_no, txt)]})
        else:
            if logs:
                logs[-1]["lines"].append((ln_no, txt))

    # 3. Parse each log into a dict
    records = []
    for rec in tqdm(logs, desc="Parsing logs"):
        ln0, first = rec["start_line"], rec["lines"][0][1]
        t = detect_type(first)
        if t == 'A':
            data = parse_type_a(first)
        elif t == 'B':
            data = parse_type_b(rec["lines"])
        elif t == 'C':
            data = parse_type_c(first)
        else:
            data = {"parse_error": "Unknown type"}
        data["line_no"] = ln0
        data["type"]    = t
        records.append(data)

    # 4. Write to Excel
    wb = xlsxwriter.Workbook(OUTPUT_FILE)
    ws = wb.add_worksheet("ParsedLogs")
    # dynamic headers
    headers = []
    for r in records:
        for k in r:
            if k not in headers:
                headers.append(k)
    for c,h in enumerate(headers):
        ws.write(0, c, h)
    for r,row in enumerate(records, start=1):
        for c,h in enumerate(headers):
            ws.write(r, c, row.get(h, ""))
    wb.close()

    print(f"Validation workbook '{OUTPUT_FILE}' created with {len(records)} rows.")

if __name__ == "__main__":
    main()
